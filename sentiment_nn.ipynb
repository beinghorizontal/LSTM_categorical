{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhf6BLtPGibZ7OZ+EvTgZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stoictrader/LSTM_categorical/blob/master/sentiment_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1cgBpCvL7zl",
        "colab_type": "code",
        "outputId": "85a2764a-6efa-4964-c865-ee8cb65d4cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q8g06dmOuxQ",
        "colab_type": "code",
        "outputId": "b6cd3f29-0764-4869-c056-6650dc79b993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "#!unzip -uq \"drive/My Drive/deeplearning/trainingandtestdata.zip\" -d \"drive/My Drive/deeplearning/tweet140\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/deeplearning/tweet140/tweet_sentiment.csv',encoding='ISO-8859-1')\n",
        "data.columns = ['sentiment','string_id','date','query','handle','text']\n",
        "data = data[['sentiment','text']]\n",
        "#data = data[data.sentiment != \"Neutral\"]\n",
        "print('before randomize ', '\\n',data['text'].head(5))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before randomize  \n",
            " 0    is upset that he can't update his Facebook by ...\n",
            "1    @Kenichan I dived many times for the ball. Man...\n",
            "2      my whole body feels itchy and like its on fire \n",
            "3    @nationwideclass no, it's not behaving at all....\n",
            "4                        @Kwesidei not the whole crew \n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRWH8IU4XSsH",
        "colab_type": "code",
        "outputId": "fac9f28c-3fa0-4380-daf8-ea733308a5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "\"remove @mentions\"\n",
        "data = data.replace(\"\\@[a-zA-Z]+\", \"\",regex=True)\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print((data['sentiment']==4).sum())\n",
        "print((data['sentiment']==0).sum())\n",
        "\n",
        "\n",
        "\"randomize data\"\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "print('after randomize, lower case, removing mentions ', '\\n',data['text'].head(5))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800000\n",
            "799999\n",
            "after randomize, lower case, removing mentions  \n",
            " 0     is am addicted to lolcats thx to  im sending ...\n",
            "1    i hate weeding shower picking up julia then wa...\n",
            "2     just remember to take off your bra during thu...\n",
            "3     apparently we dont have to do another mpn it ...\n",
            "4    this is not what a summer schedule is supposed...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ejAnHalX3C_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71EVkRDeY9kN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#......................................................... lstm .......\n",
        "def get_model(X):\n",
        "  lstm_out = 196\n",
        "  embed_dim = 128\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "  model.add(SpatialDropout1D(0.4))\n",
        "  model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(2,activation='softmax'))\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "  print(model.summary())\n",
        "  return(model)\n",
        "\n",
        "model = get_model(X=X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DtJbo5EX6Q8",
        "colab_type": "code",
        "outputId": "2780373a-f14a-4c8f-a15e-e667f8878070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\"This will convert positive and negative sentiment numbers\"\n",
        "Y =  pd.get_dummies(data['sentiment']).values\n",
        "Y[0]\n",
        "data.iloc[0]['sentiment']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.50, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(799999, 40) (799999, 2)\n",
            "(800000, 40) (800000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0A96rneYH_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "checkpointer = (ModelCheckpoint(filepath=\"/content/drive/My Drive/deeplearning/nlp.h5\", monitor='val_accuracy',\n",
        "                                verbose=1, save_best_only=True))\n",
        "\n",
        "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='max')\n",
        "\n",
        "k=0\n",
        "trainac=[]\n",
        "valac=[]\n",
        "#class_weights={0:1.,1:1.}\n",
        "batch_size = 332\n",
        "history = model.fit(X_train, Y_train, epochs = 30, batch_size=batch_size, verbose = 2,\n",
        "                    \n",
        "          validation_data=(X_test,Y_test),callbacks=[earlystopper,checkpointer])\n",
        "\n",
        "\n",
        "#epochs = 155 best trained afterwords overfit expect 65% accurecy\n",
        "\n",
        "trainl=(history.history['acc']) #, label='train')\n",
        "vall=(history.history['val_acc'])\n",
        "\n",
        "df_fit=pd.DataFrame({'trainac':trainl,'valac':vall})\n",
        "df_fit.plot()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hugIeSPValVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"retrain from where you left if colab disconnects after 12 hr\"\n",
        "new_model = get_model(X=X)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "model_path = \"/content/drive/My Drive/deeplearning/nlp.h5\"\n",
        "n_model = load_model(model_path)\n",
        "old_weights  = n_model.get_weights()\n",
        "new_model.set_weights(old_weights)\n",
        "\n",
        "batch_size = 332\n",
        "history = new_model.fit(X_train, Y_train, epochs = 30, batch_size=batch_size, verbose = 2,\n",
        "          validation_data=(X_test,Y_test),callbacks=[earlystopper,checkpointer])\n",
        "\n",
        "\n",
        "#epochs = 155 best trained afterwords overfit expect 65% accurecy\n",
        "\n",
        "trainl=(history.history['acc']) #, label='train')\n",
        "vall=(history.history['val_acc'])\n",
        "\n",
        "df_fit=pd.DataFrame({'trainac':trainl,'valac':vall})\n",
        "df_fit.plot()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}